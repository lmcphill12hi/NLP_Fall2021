{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e084271d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.8/site-packages (21.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-21.2.4-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (52.0.0.post20210125)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-58.1.0-py3-none-any.whl (816 kB)\n",
      "\u001b[K     |████████████████████████████████| 816 kB 26.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel in /opt/anaconda3/lib/python3.8/site-packages (0.36.2)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
      "Installing collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.36.2\n",
      "    Uninstalling wheel-0.36.2:\n",
      "      Successfully uninstalled wheel-0.36.2\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 52.0.0.post20210125\n",
      "    Uninstalling setuptools-52.0.0.post20210125:\n",
      "      Successfully uninstalled setuptools-52.0.0.post20210125\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.0.1\n",
      "    Uninstalling pip-21.0.1:\n",
      "      Successfully uninstalled pip-21.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 4.2.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 4.2.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\u001b[0m\n",
      "Successfully installed pip-21.2.4 setuptools-58.1.0 wheel-0.37.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#! pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74221ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e742c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tea is healthy and calming, don't you think?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d932c07c",
   "metadata": {},
   "source": [
    "## Tokenizing \n",
    "Individual unit of text in the document, such as indivuaul wokrs or punctuation. \n",
    "- don't split into do and n't\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "992cbfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea\n",
      "is\n",
      "healthy\n",
      "and\n",
      "calming\n",
      ",\n",
      "do\n",
      "n't\n",
      "you\n",
      "think\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615df21",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae9573",
   "metadata": {},
   "source": [
    "Lemmatizing: The \"lemma\" of a word is its base form. For ex, \"walk\" is the lemma of the word \"walking\".Lemmatzing \"walking\" would change it to \"walk.\"\n",
    "\n",
    "It's also common to remove stopwords. Stopwords are words that occur frequently in the language and don't contain much information. English stopwords include \"the\", \"is\", \"and\", \"but\", \"not\".\n",
    "\n",
    "With a spaCy token, token.lemma_ returns the lemma, while token.is_stop returns a boolean True if the token is a stopword (and False otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfb78247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token \t\tLemma \t\tStopword\n",
      "----------------------------------------\n",
      "Tea\t\ttea\t\tFalse\n",
      "is\t\tbe\t\tTrue\n",
      "healthy\t\thealthy\t\tFalse\n",
      "and\t\tand\t\tTrue\n",
      "calming\t\tcalm\t\tFalse\n",
      ",\t\t,\t\tFalse\n",
      "do\t\tdo\t\tTrue\n",
      "n't\t\tn't\t\tTrue\n",
      "you\t\tyou\t\tTrue\n",
      "think\t\tthink\t\tFalse\n",
      "?\t\t?\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*40)b\n",
    "for token in doc:\n",
    "    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235b8a1",
   "metadata": {},
   "source": [
    "- Romove noise from the data\n",
    "- Sometimes lemmatizing and removing stop words can decrease model performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89359a94",
   "metadata": {},
   "source": [
    "### Pattern Matching \n",
    "Another common NLP task is matching tokens or phrases within chunks of text or whole documents. You can do pattern matching with regular expressions, but spaCy's matching capabilities tend to be easier to use.\n",
    "\n",
    "To match individual tokens, you create a Matcher. When you want to match a list of terms, it's easier and more efficient to use PhraseMatcher. For example, if you want to find where different smartphone models show up in some text, you can create patterns for the model names of interest. First you create the PhraseMatcher itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d76e384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73feceb6",
   "metadata": {},
   "source": [
    "The matcher is created using the vocabulary of your model. Here we're using the small English model you loaded earlier. Setting attr='LOWER' will match the phrases on lowercased text. This provides case insensitive matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8ec4c",
   "metadata": {},
   "source": [
    "Next you create a list of terms to match in the text. The phrase matcher needs the patterns as document objects. The easiest way to get these is with a list comprehension using the nlp model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b650e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
    "patterns = [nlp(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad8704",
   "metadata": {},
   "source": [
    "Then you create a document from the text to search and use the phrase matcher to find where the terms occur in the text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "932d4329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)]\n"
     ]
    }
   ],
   "source": [
    "# Borrowed from https://daringfireball.net/linked/2019/09/21/patel-11-pro\n",
    "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n",
    "               \"photography tests pitting the iPhone 11 Pro against the \"\n",
    "               \"Galaxy Note 10 Plus and last year’s iPhone XS and Google Pixel 3.\") \n",
    "matches = matcher(text_doc)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c37bf43",
   "metadata": {},
   "source": [
    "The matches here are a tuple of the match id and the positions of the start and end of the phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c01c1a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TerminologyList iPhone 11\n"
     ]
    }
   ],
   "source": [
    "match_id, start, end = matches[0]\n",
    "print(nlp.vocab.strings[match_id], text_doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49a24d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iPhone 11"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[0]\n",
    "text_doc[start:end]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
